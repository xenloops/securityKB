# Artificial Intelligence / Large Language Model Security

AI and LLMs have revolutionized various industries by providing advanced capabilities such as natural language understanding, predictive analysis, and decision support. However, the increasing integration of AI/LLM technologies into critical systems has also heightened concerns about their security. Securing these systems is essential to protect sensitive data, ensure trust, and maintain the reliability and integrity of AI-driven applications.

The following outlines key security requirements for AI/LLM systems, providing a foundation for mitigating risks and safeguarding these technologies. Fortunately, many of the controls that apply to AI are the same controls in use for decades to secure applications.

## Educate

* Provide guidance on avoiding the input of sensitive information. Offer training on best practices for interacting with LLMs securely.
* Maintain clear policies about data retention, usage, and deletion. 



## Access controls

* Enforce strict user authentication and access controls to prevent unauthorized interactions with the AI/LLM system. This reduces the risk of misuse or exposure to malicious actors.
* Safeguard APIs used to interact with LLMs by implementing HTTPS protocols, API keys, rate limiting, and activity logging to prevent abuse or unauthorized access.
* Implement human-in-the-loop controls for privileged operations to prevent unauthorized actions.


## Data controls

* Many AI/LLM systems process sensitive data during training and inference. It is crucial to ensure compliance with privacy regulations (e.g., GDPR, CCPA) by implementing robust data anonymization, encryption, and access controls.
* Maintain a clear record of the data used for training and the decisions made by the LLM to support traceability, accountability, and debugging in the event of issues.
* Implement mechanisms to sanitize and validate outputs to prevent the dissemination of harmful, biased, or sensitive information generated by the model.
* Ensure that the LLM adheres to ethical standards by minimizing biases, preventing harm, and avoiding outputs that could lead to legal or reputational issues.
* Specify clear output formats, request detailed reasoning and source citations, and use deterministic code to validate adherence to these formats.
* Implement input and output filtering: Apply semantic filters and use string-checking to scan for restricted and sensitive content. Evaluate responses using the RAG Triad: Assess context relevance, groundedness, and question/answer relevance to identify potentially malicious outputs.
* Separate and clearly denote untrusted content to limit its influence on user prompts.
* Limit model access to external data sources, and ensure runtime data orchestration is securely managed to avoid unintended data leakage.
* Train models using decentralized data stored across multiple servers or devices to minimize the need for centralized data collection and reduce exposure risks.
* Apply techniques that add noise to the data or outputs to make it difficult for attackers to reverse-engineer individual data points.
* Allow users to opt out of having their data included in training processes.
* Use homomorphic encryption to enable secure data analysis and privacy-preserving machine learning to ensure that data remains confidential while being processed by the model.
* Implement tokenization or redaction to preprocess and sanitize sensitive information. Techniques like pattern matching can detect and redact confidential content before processing.




## System Integrity

* Protect the LLM from unauthorized modifications or adversarial attacks that could compromise its behavior. Use cryptographic hashing and verification techniques to maintain the integrity of the model files.
* Secure the runtime environment where the LLM is deployed. This includes containerization, network segmentation, and regular updates to prevent vulnerabilities in underlying infrastructure.
* Use encryption and digital rights management when sharing models or collaborating across organizations to prevent unauthorized use or tampering.
* Deploy defenses such as query rate limits and response obfuscation to prevent attackers from reconstructing the LLM through repeated queries.
* Enforce strict context adherence, limit responses to specific tasks or topics, and instruct the model to ignore attempts to modify core instructions.
* Prevent leaking sensitive information through error messages or configuration details.
* 

## Supply chain

* Carefully vet data sources and suppliers. Regularly review and audit supplier security and access for changes in their security posture or terms and conditions.
* Employ vulnerability scanning, management, and patching components. For development environments with access to sensitive data, apply these controls in those environments, too.




## Monitoring

* Employ systems to monitor for unusual activities or anomalies during LLM operation. This can include detecting abnormal request patterns or unexpected model outputs.

## Testing

* Conduct adversarial testing and attack simulations: perform regular penetration testing and breach simulations, treating the model as an untrusted user to test the effectiveness of trust boundaries and access controls.



## Conclusion

Addressing these security requirements ensures that AI/LLM systems remain trustworthy and resilient against evolving threats. As these technologies continue to advance, ongoing vigilance and updates to security practices are essential to mitigate emerging risks and maintain user confidence.


## References

* [MITRE](https://atlas.mitre.org/). "Adversarial Threat Landscape for Artificial-Intelligence Systems (ATLAS)." Framework for understanding and mitigating adversarial threats in AI systems.
* [National Institute of Standards and Technology (NIST)](https://www.nist.gov/itl/ai-risk-management-framework). "AI Risk Management Framework." Provides guidelines for managing AI risks, including security and ethical considerations.
* [OpenAI](https://platform.openai.com/docs). "OpenAI API: Safety and Security Best Practices." Documentation on API security practices for deploying LLMs safely.
* [Google AI](https://ai.google/responsibility/principles/). "Responsible AI Practices." Offers guidance on creating and deploying AI systems ethically and securely.
* [Microsoft](https://www.microsoft.com/ai). "Securing AI Systems." A whitepaper detailing the strategies for protecting AI systems against threats.
* [ISO/IEC 27001](https://www.iso.org/standard/27001). "Information Security Management Systems." International standard for managing information security risks, applicable to AI data handling.
* [European Commission. (2018)](https://gdpr-info.eu/). "General Data Protection Regulation (GDPR)." Outlines requirements for data privacy and protection, applicable to AI systems.
* [Goodfellow, I., Shlens, J., & Szegedy, C. (2015)](https://arxiv.org/abs/1412.6572). "Explaining and Harnessing Adversarial Examples." Discusses adversarial attacks and defenses relevant to machine learning models.
* [Papernot, N., McDaniel, P., et al. (2016)](https://arxiv.org/abs/1511.04508). "Distillation as a Defense to Adversarial Perturbations." Explores techniques for defending machine learning models against adversarial attacks.
* [The Partnership on AI](https://www.partnershiponai.org/). "Tenets for Responsible AI." A set of principles for developing AI systems that prioritize safety and ethics.

