# Artificial Intelligence / Large Language Model Security

AI and LLMs have revolutionized various industries by providing advanced capabilities such as natural language understanding, predictive analysis, and decision support. However, the increasing integration of AI/LLM technologies into critical systems has also heightened concerns about their security. Securing these systems is essential to protect sensitive data, ensure trust, and maintain the reliability and integrity of AI-driven applications.

The following outlines key security requirements for AI/LLM systems, providing a foundation for mitigating risks and safeguarding these technologies. Fortunately, many of the controls that apply to AI are the same controls in use for decades to secure applications.

## Education

* Provide guidance on avoiding the input of sensitive information. Offer training on best practices for interacting with LLMs securely.
* Maintain clear policies about data retention, usage, and deletion. 



## Access Controls

* Enforce strict user authentication and access controls to prevent unauthorized interactions with the AI/LLM system. This reduces the risk of misuse or exposure to malicious actors.
* Safeguard APIs used to interact with LLMs by implementing HTTPS protocols, API keys, rate limiting, and activity logging to prevent abuse or unauthorized access.
* Implement human-in-the-loop controls for privileged operations to prevent unauthorized actions.
* Implement authorization in downstream systems rather than relying on the LLM to decide whether an action is allowed. Enforce the complete mediation principle so that all requests made to downstream systems via extensions are validated against security policies.
* Limit the permissions that the system or extensions it uses are granted to other systems to the minimum necessary.
* Ensure actions taken on behalf of a user are executed on downstream systems in the context of that specific user, and with the minimum privileges necessary.


## Data Controls

* Many AI/LLM systems process sensitive data during training and inference. It is crucial to ensure compliance with privacy regulations (e.g., GDPR, CCPA) by implementing robust data anonymization, encryption, and access controls.
* Maintain a clear record of the data used for training and the decisions made by the LLM to support traceability, accountability, and debugging in the event of issues.
* Implement mechanisms to sanitize and validate outputs to prevent the dissemination of harmful, biased, or sensitive information generated by the model.
* Ensure that the LLM adheres to ethical standards by minimizing biases, preventing harm, and avoiding outputs that could lead to legal or reputational issues.
* Specify clear output formats, request detailed reasoning and source citations, and use deterministic code to validate adherence to these formats.
* Implement input and output filtering: Apply semantic filters and use string-checking to scan for restricted and sensitive content. Evaluate responses using the RAG Triad: Assess context relevance, groundedness, and question/answer relevance to identify potentially malicious outputs.
* Separate and clearly denote untrusted content to limit its influence on user prompts.
* Limit model access to external data sources, and ensure runtime data orchestration is securely managed to avoid unintended data leakage.
* Train models using decentralized data stored across multiple servers or devices to minimize the need for centralized data collection and reduce exposure risks.
* Apply techniques that add noise to the data or outputs to make it difficult for attackers to reverse-engineer individual data points.
* Allow users to opt out of having their data included in training processes.
* Use homomorphic encryption to enable secure data analysis and privacy-preserving machine learning to ensure that data remains confidential while being processed by the model.
* Implement tokenization or redaction to preprocess and sanitize sensitive information. Techniques like pattern matching can detect and redact confidential content before processing.
* Track data origins and transformations and verify data legitimacy during all model development stages.
* Validate model outputs against trusted sources to detect signs of model poisoning.
* Implement strict sandboxing to limit model exposure to unverified data sources. Use anomaly detection techniques to filter out adversarial data.
* Use data version control (DVC) to track changes in datasets and detect manipulation to maintain model integrity.
* Store user-supplied information in a vector database, allowing adjustments without re-
training the entire model.
* During inference, integrate Retrieval-Augmented Generation (RAG) and grounding techniques to reduce risks of hallucinations.
* Treat the model as a user, adopting a zero-trust approach; apply proper input validation on responses from the model to backend functions.
* Employ standard secure software controls (e.g. context-aware output encoding, parameterized queries/prepared statements for database operations, cross-site scripting protections, strict Content Security Policies (CSP)) involving LLM output.

## Model Controls
* Thoroughly vet any extensions which include functions that are not needed for the intended operation of the system (e.g. an extension used includes the ability to modify and delete documents in a repository or run a specific shell command but fails to prevent other shell commands).
* Restrict extensions' permissions on downstream systems that are not needed for the
system (e.g. an extension intended to read data connects to a database using an identity that also has UPDATE, INSERT, and DELETE permissions).
* Employ independent verification for high-impact actions (e.g. models or extensions that allow a user's documents to be deleted performs deletions without any confirmation from the user).
* Limit the extensions that LLM agents are allowed to call to only the minimum necessary (e.g. a system that does not require the ability to fetch the contents of a URL should not have that functionality or extension).
* Limit the functions implemented in LLM extensions to the minimum necessary.
* Avoid the use of open-ended extensions where possible (e.g. run a shell command or fetch a
URL) and use extensions with granular functionality.

## Prompt Controls
* Avoid embedding any sensitive information (e.g. API keys, auth keys, database names, user roles, permission structure of the application) directly in the system prompts. Externalize such information to the systems that the model does not directly access.
* Avoid using system prompts to control the model behavior where possible. Instead, rely on systems outside of the LLM to ensure this behavior (e.g. detecting and preventing harmful content should be done in external systems).
* Implement a system of guardrails outside of the LLM itself. Training particular behavior into a model does not guarantee that the model will always adhere to the guard. Use an independent system to inspect the output and determine if the model is in compliance with expectations.
* Ensure that security controls are enforced independently from the LLM (e.g. privilege separation or authorization bounds checks), and occur in a deterministic, auditable manner (which LLMs are not conducive to). In cases where an agent is performing tasks, multiple agents should be used, each configured with the least privileges needed to perform the desired tasks.


## System Integrity

* Protect the LLM from unauthorized modifications or adversarial attacks that could compromise its behavior. Use cryptographic hashing and verification techniques to maintain the integrity of the model files.
* Secure the runtime environment where the LLM is deployed. This includes containerization, network segmentation, and regular updates to prevent vulnerabilities in underlying infrastructure.
* Use encryption and digital rights management when sharing models or collaborating across organizations to prevent unauthorized use or tampering.
* Deploy defenses such as query rate limits and response obfuscation to prevent attackers from reconstructing the LLM through repeated queries.
* Enforce strict context adherence, limit responses to specific tasks or topics, and instruct the model to ignore attempts to modify core instructions.
* Prevent leaking sensitive information through error messages or configuration details.
* 

## Supply Chain Controls

* Carefully vet data sources and suppliers. Regularly review and audit supplier security and access for changes in their security posture or terms and conditions.
* Employ vulnerability scanning, management, and patching components. For development environments with access to sensitive data, apply these controls in those environments, too.
* Maintain a system component inventory using a Software Bill of Materials (SBOM) to
ensure there is an up-to-date, accurate, and signed inventory of components, preventing tampering with deployed packages or known vulnerabilities.
* Maintain a system component license inventory and conduct regular audits of all software, tools, and datasets to ensure compliance and transparency.
* Use models only from verifiable sources and use model integrity checks with signing and file hashes to compensate for the lack of strong model provenance. Use code signing for externally supplied code.
* Implement a patching policy to mitigate vulnerable or outdated components. Ensure the system uses a maintained version of APIs and the underlying model.
* Encrypt models with integrity checks and use vendor attestation APIs to prevent tampered apps and models and terminate applications of unrecognized software.



## Logging and Monitoring

* Employ systems to monitor for unusual activities or anomalies during LLM operation. This can include detecting abnormal request patterns or unexpected model outputs.
* Implement strict monitoring and auditing practices for collaborative model development
environments to prevent and quickly detect any abuse.

## Testing

* Conduct adversarial testing and attack simulations: perform regular penetration testing and breach simulations, treating the model as an untrusted user to test the effectiveness of trust boundaries and access controls.
* Employ anomaly detection and adversarial robustness tests on supplied models and data can help
detect tampering and poisoning.
* Test model robustness with red team campaigns and adversarial techniques, such as federated learning, to minimize the impact of data perturbations.
* Monitor training loss and analyze model behavior for signs of poisoning. Use thresholds to
detect anomalous outputs.


## Conclusion

Addressing these security requirements ensures that AI/LLM systems remain trustworthy and resilient against evolving threats. As these technologies continue to advance, ongoing vigilance and updates to security practices are essential to mitigate emerging risks and maintain user confidence.


## References

* [MITRE](https://atlas.mitre.org/). "Adversarial Threat Landscape for Artificial-Intelligence Systems (ATLAS)." Framework for understanding and mitigating adversarial threats in AI systems.
* [National Institute of Standards and Technology (NIST)](https://www.nist.gov/itl/ai-risk-management-framework). "AI Risk Management Framework." Provides guidelines for managing AI risks, including security and ethical considerations.
* [OpenAI](https://platform.openai.com/docs). "OpenAI API: Safety and Security Best Practices." Documentation on API security practices for deploying LLMs safely.
* [Google AI](https://ai.google/responsibility/principles/). "Responsible AI Practices." Offers guidance on creating and deploying AI systems ethically and securely.
* [Microsoft](https://www.microsoft.com/ai). "Securing AI Systems." A whitepaper detailing the strategies for protecting AI systems against threats.
* [ISO/IEC 27001](https://www.iso.org/standard/27001). "Information Security Management Systems." International standard for managing information security risks, applicable to AI data handling.
* [European Commission. (2018)](https://gdpr-info.eu/). "General Data Protection Regulation (GDPR)." Outlines requirements for data privacy and protection, applicable to AI systems.
* [Goodfellow, I., Shlens, J., & Szegedy, C. (2015)](https://arxiv.org/abs/1412.6572). "Explaining and Harnessing Adversarial Examples." Discusses adversarial attacks and defenses relevant to machine learning models.
* [Papernot, N., McDaniel, P., et al. (2016)](https://arxiv.org/abs/1511.04508). "Distillation as a Defense to Adversarial Perturbations." Explores techniques for defending machine learning models against adversarial attacks.
* [The Partnership on AI](https://www.partnershiponai.org/). "Tenets for Responsible AI." A set of principles for developing AI systems that prioritize safety and ethics.

